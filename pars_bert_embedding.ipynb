{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOIIwwWqdFztLtR1Xs1aCr+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"yMbvtJH6TDOk"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GyGE0N2_rRnx"},"outputs":[],"source":["!pip install -q transformers\n","!pip install -q hazm\n","!pip install -q PersianStemmer\n","!pip install -q clean-text[gpl]\n","!pip install nltk"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7n8NlI7nH_a7"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TJizpKAwHEmJ"},"outputs":[],"source":["%cd /content/drive/MyDrive/models/persian_talent/claim"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8HddOv9pIPL0"},"outputs":[],"source":["import pandas as pd\n","data_train = pd.read_csv('-923066623_-294158543.csv')\n","data_train\n","data2 = pd.read_csv('153952002_-1390683709 (1).csv')\n","data_train = pd.concat([data_train,data2],ignore_index=True)\n","data_train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R-fVEc1mMbRv"},"outputs":[],"source":["\n","data_test = pd.read_csv('4.test_data_without_label_text.csv')\n","\n","data_test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-wA-MPnPjqnB"},"outputs":[],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jAwhUnS8rnPL"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import nltk\n","import re\n","nltk.download('punkt')\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import f1_score\n","from sklearn.utils import shuffle\n","from PersianStemmer import PersianStemmer\n","from hazm import *\n","from cleantext import clean\n","\n","from transformers import BertConfig, BertTokenizer\n","from transformers import TFBertModel, TFBertForSequenceClassification\n","from transformers import glue_convert_examples_to_features\n","from transformers import AutoConfig, AutoTokenizer, AutoModel, TFAutoModel\n","\n","import tensorflow_datasets as tfds\n","import tensorflow as tf\n","\n","from tqdm.notebook import tqdm\n","\n","import os\n","import re\n","import json\n","import copy\n","import collections"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mdkXaTaFN6VZ"},"outputs":[],"source":["def clean_sentence(sentence):\n","    sentence = arToPersianChar(sentence)\n","    sentence = arToPersianNumb(sentence)\n","    sentence = remove_number(sentence)\n","    # more_normalization_function()\n","    return sentence\n","\n","\n","def arToPersianNumb(number):\n","    dic = {\n","        '١': '۱',\n","        '٢': '۲',\n","        '٣': '۳',\n","        '٤': '۴',\n","        '٥': '۵',\n","        '٦': '۶',\n","        '٧': '۷',\n","        '٨': '۸',\n","        '٩': '۹',\n","        '٠': '۰',\n","        '1': '۱',\n","        '2': '۲',\n","        '3': '۳',\n","        '4': '۴',\n","        '5': '۵',\n","        '6': '۶',\n","        '7': '۷',\n","        '8': '۸',\n","        '9': '۹',\n","        '0': '۰',\n","\n","    }\n","    return multiple_replace(dic, number)\n","\n","def remove_number(number):\n","    dic = {\n","        '۱': '',\n","        '۲': '',\n","        '۳': '',\n","        '۴': '',\n","        '۵': '',\n","        '۶': '',\n","        '۷': '',\n","        '۸': '',\n","        '۹': '',\n","        '۰': '',\n","\n","    }\n","    return multiple_replace(dic, number)\n","\n","def arToPersianChar(userInput):\n","    dic = {\n","        'ك': 'ک',\n","        'دِ': 'د',\n","        'بِ': 'ب',\n","        'زِ': 'ز',\n","        'ذِ': 'ذ',\n","        'شِ': 'ش',\n","        'سِ': 'س',\n","        'ى': 'ی',\n","        'ي': 'ی'\n","        }\n","    return multiple_replace(dic, userInput)\n","\n","\n","def multiple_replace(dic, text):\n","    pattern = \"|\".join(map(re.escape, dic.keys()))\n","    return re.sub(pattern, lambda m: dic[m.group()], str(text))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jh8J6UtoZZvL"},"outputs":[],"source":["def clean_all(document):\n","    clean = []\n","    for sentence in document:\n","        sentence = clean_sentence(sentence)\n","        clean.append(sentence)\n","    return clean"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HJz4MDF7eSM7"},"outputs":[],"source":["pesrian_stopwords = []\n","with open('../persian') as f:\n","    persian_stopwords = f.readlines()\n","\n","\n","persian_stopwords = [i[:-1]for i in persian_stopwords]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HvKsPQveTLzG"},"outputs":[],"source":["persian_stopwords"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uUpfbIfLrm2G"},"outputs":[],"source":["lines = data_train.data.tolist()\n","\n","lines = clean_all(lines)\n","normalizer = Normalizer()\n","lines = list(map(lambda line: normalizer.normalize(line), lines))\n","ps = PersianStemmer()\n","lemmatizer = Lemmatizer()\n","\n","\n","train = []\n","\n","target = []\n","for line in lines:\n","    line = re.sub(r\"[_,-؛,،؟?!/*#]\",' ',line)\n","    line = ps.run(line)\n","    words = nltk.tokenize.word_tokenize(line)\n","    #words = [re.sub(r\"[_,-?!/]\",' ',word) for word in words]\n","    #words = [ps.run(word) for word in words]\n","    words = [word for word in words  if word not in persian_stopwords]\n","    #words = [lemmatizer.lemmatize(word) for word in words]\n","    train.append(words)\n"]},{"cell_type":"code","source":["lines = data_test.data.tolist()\n","\n","lines = clean_all(lines)\n","normalizer = Normalizer()\n","lines = list(map(lambda line: normalizer.normalize(line), lines))\n","ps = PersianStemmer()\n","lemmatizer = Lemmatizer()\n","\n","\n","test = []\n","\n","target = []\n","for line in lines:\n","    line = re.sub(r\"[_,-؛,،؟?!/*#]\",' ',line)\n","    line = ps.run(line)\n","    words = nltk.tokenize.word_tokenize(line)\n","    #words = [re.sub(r\"[_,-?!/]\",' ',word) for word in words]\n","    #words = [ps.run(word) for word in words]\n","    words = [word for word in words  if word not in persian_stopwords]\n","    #words = [lemmatizer.lemmatize(word) for word in words]\n","    test.append(words)\n"],"metadata":{"id":"lbwJGDXnL8wt"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CvjZ52HsVaBy"},"outputs":[],"source":["print('sample of data befor cleaning:{}'.format(lines[1]))\n","print('sample of cleaned data:{}'.format(test[1]))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0XMGLnzmOlXj"},"outputs":[],"source":["all_words = set([word for sentence in train for word in sentence])\n","num_words = len(set([word for sentence in train for word in sentence]))\n","\n","labels = data_train.columns[3:].tolist()\n","target = data_train[labels]\n","num_tags   = len(set(labels))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8vQA-t5JWZJY"},"outputs":[],"source":["target"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sjqF8Ol8Phrx"},"outputs":[],"source":["print(\"Total number of tagged sentences: {}\".format(len(train)))\n","print(\"Vocabulary size: {}\".format(num_words))\n","print(\"Total number of tags: {}\".format(num_tags))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"63EIafZsJuAL"},"outputs":[],"source":["d = []\n","for line in train:\n","  ds = ''\n","  for i in range(len(line)):\n","    ds += line[i] +' '\n","  d.append(ds)\n","\n","train = pd.DataFrame(data=d,columns=['topic'])\n","train[labels] = target"]},{"cell_type":"code","source":["d = []\n","for line in test:\n","  ds = ''\n","  for i in range(len(line)):\n","    ds += line[i] +' '\n","  d.append(ds)\n","\n","test = pd.DataFrame(data=d,columns=['topic'])\n"],"metadata":{"id":"5pa2-lIlMqnX"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9STSwP40JuEt"},"outputs":[],"source":["train.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ge9fApiNGqYT"},"outputs":[],"source":["from transformers import AutoConfig, AutoTokenizer, AutoModel\n","import torch\n","from tqdm import tqdm\n","\n","config = AutoConfig.from_pretrained(\"HooshvareLab/bert-base-parsbert-uncased\")\n","tokenizer = AutoTokenizer.from_pretrained(\"HooshvareLab/bert-base-parsbert-uncased\")\n","model = AutoModel.from_pretrained(\"HooshvareLab/bert-base-parsbert-uncased\")\n","\n","bert_features=[]\n","for text in tqdm(train['topic']):\n","\n","  input_ids = tokenizer.encode(text, add_special_tokens=True)\n","  input_ids = torch.tensor([input_ids])\n","\n","  # Pass the tokenized input through BERT\n","  outputs = model(input_ids)\n","\n","  # Extract features from the last layer\n","  last_layer_features = outputs.last_hidden_state.detach().numpy()\n","  bert_features.append(last_layer_features[0])\n"]},{"cell_type":"code","source":["test_bert_features=[]\n","for text in tqdm(test['topic']):\n","\n","  input_ids = tokenizer.encode(text, add_special_tokens=True)\n","  input_ids = torch.tensor([input_ids])\n","\n","  # Pass the tokenized input through BERT\n","  outputs = model(input_ids)\n","\n","  # Extract features from the last layer\n","  last_layer_features = outputs.last_hidden_state.detach().numpy()\n","  test_bert_features.append(last_layer_features[0])\n"],"metadata":{"id":"tu9rk2_mM3fS"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q6-CAqgKYJUO"},"outputs":[],"source":["bert_features[1].shape"]},{"cell_type":"code","source":["bert_pad_size = [len(i) for i in bert_features]\n","train_pad = [np.pad(i,((0,max(bert_pad_size)-len(i)),(0,0))) for i in bert_features]\n","test_pad = [np.pad(i,((0,max(bert_pad_size)-len(i)),(0,0))) for i in test_bert_features]\n"],"metadata":{"id":"mY40vddxBCNc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["max(bert_pad_size)"],"metadata":{"id":"mzVbCKIV7d1b"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R9JL_WHAXgKP"},"outputs":[],"source":["train_features = pd.DataFrame()\n","train_features['bert_features'] = train_pad"]},{"cell_type":"code","source":["test_features = pd.DataFrame()\n","test_features['bert_features'] = test_pad"],"metadata":{"id":"nDHNXJjLNWTz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_features"],"metadata":{"id":"OAruAHFcAvy9"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x_NeFRQFCd-3"},"outputs":[],"source":["#!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fa.300.vec.gz\n","#!gzip -d cc.fa.300.vec.gz\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dNScpV3kChZW"},"outputs":[],"source":["from gensim.models import KeyedVectors\n","\n","model_path = \"../cc.fa.300.vec\"\n","word2vec_model = KeyedVectors.load_word2vec_format(model_path, binary=False)\n","\n","\n"]},{"cell_type":"code","source":["train"],"metadata":{"id":"0XWTmmWP7Q_R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["word_embedding =[]\n","for text in train['topic']:\n","  vector=[]\n","  words = nltk.tokenize.word_tokenize(text)\n","  for word in words:\n","    try:\n","      vector.append(word2vec_model[word])\n","    except:\n","      0\n","  word_embedding.append(vector)\n"],"metadata":{"id":"HVvL711KACCu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_word_embedding =[]\n","for text in test['topic']:\n","  vector=[]\n","  words = nltk.tokenize.word_tokenize(text)\n","  for word in words:\n","    try:\n","      vector.append(word2vec_model[word])\n","    except:\n","      0\n","  test_word_embedding.append(vector)\n"],"metadata":{"id":"0THwhgTzNidG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["w_pad_size = [len(i) for i in word_embedding]\n","train_w_pad = [np.pad(i,((0,max(w_pad_size)-len(i)),(0,0))) for i in word_embedding]\n","test_w_pad = [np.pad(i,((0,max(w_pad_size)-len(i)),(0,0))) for i in test_word_embedding]"],"metadata":{"id":"AtqQONbBA6Xh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(train_w_pad[2])"],"metadata":{"id":"lHLBquHuFrww"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["target.to_numpy()"],"metadata":{"id":"Y9IajOX_HQBh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_features['word_embedding'] = train_w_pad\n","train_features['label'] = list(target.to_numpy())\n","train_features.to_pickle('train_features.pkl')\n","train_features"],"metadata":{"id":"jVVIRPqsHByW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_features = pd.read_pickle('train_features.pkl')\n","train_features"],"metadata":{"id":"E7SxTWIp_Dbm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_features['word_embedding'] = test_w_pad\n","test_features"],"metadata":{"id":"k8BcM99ZN2YX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_features.to_pickle('test_features.pkl')\n"],"metadata":{"id":"7lw7p_-JxXCu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O5j4dh4pLBsZ"},"source":["## loss weight"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DUnB-OzuLnOR"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import tensorflow.keras.backend as K\n"]},{"cell_type":"code","source":["def calculating_class_weights(y_true):\n","    from sklearn.utils.class_weight import compute_class_weight\n","    number_dim = np.shape(y_true)[1]\n","    weights = np.empty([number_dim, 2])\n","    for i in range(number_dim):\n","        weights[i] = compute_class_weight(class_weight ='balanced', classes=[0.,1.],  y=y_true[:, i])\n","    return weights\n","\n","def get_weighted_loss(weights):\n","    def weighted_loss(y_true, y_pred):\n","        return K.mean((weights[:,0]**(1-y_true))*(weights[:,1]**(y_true))*K.binary_crossentropy(y_true, y_pred), axis=-1)\n","    return weighted_loss\n","\n"],"metadata":{"id":"gF_FN052TzAi"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5Ix-EMolLAfS"},"outputs":[],"source":["from tensorflow.keras.layers import Input, Conv1D, BatchNormalization, Dropout, Flatten, Dense\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.regularizers import l2\n","import tensorflow as tf\n","import numpy as np\n","\n","def create_cnn_model(input_shape):\n","\n","    droprate = 0.2\n","    input_layer = Input(shape=input_shape)\n","\n","    # CNN layers\n","    conv1 = Conv1D(filters=128, kernel_size=3, activation='relu')(input_layer)\n","    batch1 = BatchNormalization()(conv1)\n","    drop1 = Dropout(droprate)(batch1)\n","\n","    conv2 = Conv1D(filters=128, kernel_size=3, activation='relu')(drop1)\n","    batch2 = BatchNormalization()(conv2)\n","    drop2 = Dropout(droprate)(batch2)\n","\n","    conv3 = Conv1D(filters=128, kernel_size=3, activation='relu')(drop2)\n","    batch3 = BatchNormalization()(conv3)\n","    drop3 = Dropout(droprate)(batch3)\n","\n","\n","    # Flatten layer\n","    flatten = Flatten()(drop3)\n","\n","    # Classification layers\n","    dense = Dense(128, activation='relu')(flatten)\n","    batch4 = BatchNormalization()(dense)\n","    drop4 = Dropout(droprate)(batch4)\n","\n","    dense2 = Dense(128, activation='relu')(drop4)\n","    dense3 = Dense(64, activation='relu')(dense2)\n","\n","\n","    # Output layer\n","    outputs = Dense(11, activation='sigmoid')(dense3)\n","    model = Model(inputs=input_layer, outputs=outputs)\n","\n","\n","    model.compile(optimizer = tf.keras.optimizers.Adam(0.0001),\n","              loss = get_weighted_loss(class_weights),\n","              metrics = ['accuracy',tf.keras.metrics.AUC()])\n","\n","    return model\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","source":["from tensorflow.keras.layers import Input, Conv1D, BatchNormalization, Dropout, Flatten, Dense\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.regularizers import l2\n","from tensorflow.keras import regularizers\n","import tensorflow as tf\n","import numpy as np\n","\n","def create_cnn_model(input_shape):\n","\n","    droprate = 0.2\n","    input_layer = Input(shape=input_shape)\n","\n","    # CNN layers\n","    conv1 = Conv1D(filters=8, kernel_size=10, activation='relu')(input_layer)\n","    batch1 = BatchNormalization()(conv1)\n","    drop1 = Dropout(droprate)(batch1)\n","\n","\n","    # Flatten layer\n","    flatten = Flatten()(drop1)\n","\n","    # Classification layers\n","    dense = Dense(64, activation='relu',bias_regularizer=regularizers.L2(1e-4),kernel_regularizer=regularizers.l2(0.01))(flatten)\n","    batch4 = BatchNormalization()(dense)\n","    drop4 = Dropout(droprate)(batch4)\n","\n","    dense2 = Dense(64, activation='relu',bias_regularizer=regularizers.L2(1e-4),kernel_regularizer=regularizers.l2(0.01))(drop4)\n","    dense3 = Dense(64, activation='relu',bias_regularizer=regularizers.L2(1e-4),kernel_regularizer=regularizers.l2(0.01))(dense2)\n","\n","\n","    # Output layer\n","    outputs = Dense(11, activation='sigmoid')(dense3)\n","    model = Model(inputs=input_layer, outputs=outputs)\n","\n","\n","    model.compile(optimizer = tf.keras.optimizers.Adam(0.0001),\n","              loss = get_weighted_loss(class_weights),\n","              metrics = ['accuracy',tf.keras.metrics.AUC()])\n","\n","    return model\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"sG3aJ_Yhyh7I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Bidirectional,LSTM, Dense\n","from tensorflow.keras.optimizers import Adam\n","\n","def create_cnn_model(input_shape):\n","\n","    droprate = 0.2\n","    input_layer = Input(shape=input_shape)\n","\n","    # CNN layers\n","\n","\n","\n","    conv3 = Bidirectional(LSTM(128))(input_layer)\n","    batch3 = BatchNormalization()(conv3)\n","    drop3 = Dropout(droprate)(batch3)\n","\n","\n","    # Flatten layer\n","    flatten = Flatten()(drop3)\n","\n","    # Classification layers\n","    dense = Dense(128, activation='relu')(flatten)\n","    batch4 = BatchNormalization()(dense)\n","    drop4 = Dropout(droprate)(batch4)\n","\n","    dense2 = Dense(128, activation='relu')(drop4)\n","    dense3 = Dense(64, activation='relu')(dense2)\n","\n","\n","    # Output layer\n","    outputs = Dense(11, activation='sigmoid')(dense3)\n","    model = Model(inputs=input_layer, outputs=outputs)\n","\n","\n","    model.compile(optimizer = tf.keras.optimizers.Adam(0.0001),\n","              loss = get_weighted_loss(class_weights),\n","              metrics = ['accuracy',tf.keras.metrics.AUC()])\n","\n","    return model\n"],"metadata":{"id":"Wcv2_zBfyycX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_features['bert_features'] = list(np.array(train_features['bert_features'].to_list())[:,0,:].reshape(-1,768,1))\n"],"metadata":{"id":"3pq0EfdC9tG2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train, valid = train_test_split(train_features, test_size=0.15)"],"metadata":{"id":"zHloHCOlXnSA"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qWTMNxZSLAmH"},"outputs":[],"source":["models = ['bert_features','word_embedding']\n","\n","from sklearn.utils import shuffle\n","\n","\n","for index,model_name in enumerate(models):\n","  X_train = np.array(train[model_name].tolist())\n","  y_train = np.array(train['label'].tolist())\n","  X_val = np.array(valid[model_name].tolist())\n","  y_val = np.array(valid['label'].tolist())\n","  y_train = y_train.astype(np.float32)\n","  y_val = y_val.astype(np.float32)\n","\n","  class_weights = calculating_class_weights(y_train)\n","\n","\n","  input_shape = X_train.shape[1:]  # Replace with your actual input shape\n","\n","  model = create_cnn_model(input_shape)\n","\n","\n","  model.fit(X_train, y_train, batch_size = 64, epochs = 100, validation_data = [X_val, y_val],\n","                      callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=5)])\n","  model.save_weights(model_name+\".h5\")\n"]},{"cell_type":"code","source":["import tensorflow as tf\n","from keras.layers import Dense, Flatten,BatchNormalization,Input,Activation,Dropout\n","from keras.models import Sequential,Model\n","from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping\n","from tensorflow.keras.regularizers import l2\n","from tensorflow.keras import regularizers\n","from sklearn.model_selection import GridSearchCV\n","from tensorflow.keras.models import Sequential\n","#from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM\n","\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.applications import mobilenet_v2, mobilenet, resnet50, densenet\n","from tensorflow.keras.layers import Dense, MaxPooling2D, Conv2D, Flatten, \\\n","    BatchNormalization, Activation, GlobalAveragePooling2D, DepthwiseConv2D, \\\n","    Dropout, ReLU, Concatenate, Input, add, Conv1D, MaxPooling1D\n","from tensorflow.keras.layers.experimental.preprocessing import Normalization\n","from tensorflow.keras.layers import LSTM, GRU, SimpleRNN,Reshape\n","from tensorflow.keras.utils import plot_model\n","from sklearn.utils import shuffle\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","from tensorflow.keras import backend as K\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","import math\n","from keras.regularizers import l1, l2\n","from tensorflow.keras.callbacks import CSVLogger\n","from datetime import datetime\n","from tensorflow.keras.losses import BinaryCrossentropy\n","import numpy as np\n","from keras.models import Model\n","from keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, concatenate\n","from keras.utils import to_categorical\n","from sklearn.model_selection import train_test_split\n"],"metadata":{"id":"r35p8-T9fKYa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from keras.models import load_model\n","\n","def load_all_models(models_name,input_shape):\n","  model_list = list()\n","  for index,i in enumerate(models_name):\n","    name = i+'.h5'\n","    model = create_cnn_model(input_shape[index])\n","    model.load_weights(name)\n","    model_list.append(model)\n","    print('loaded %s'%i)\n","  return model_list\n","\n","\n","def define_stacked_model(members):\n","\twd = 0.005\n","\tdroprate=0.2\n","\t# update all layers in all models to not be trainable\n","\tfor i in range(len(members)):\n","\t\tmodel = members[i]\n","\t\tfor layer in model.layers:\n","\t\t\tlayer.trainable = True\n","\t\t\t# rename to avoid 'unique layer name' issue\n","\t\t\tlayer._name = 'ensemble_' + str(i+1) + '_' + layer.name\n","\n","\t# define multi-headed input\n","\tensemble_visible = [model.input for model in members]\n","\t# concatenate merge output from each model\n","\tensemble_outputs = [model.output for model in members]\n","\tmerge = concatenate(ensemble_outputs)\n","\thidden_dense = Dense(64,kernel_regularizer=l2(wd), activation='relu')(merge)\n","\toutput = Dense(11, activation='sigmoid')(hidden_dense)\n","\tmodel = Model(inputs=ensemble_visible, outputs=output)\n","\t# plot graph of ensemble\n","\tplot_model(model, show_shapes=True, to_file='model_graph.png')\n","\t# compile\n","\tmodel.compile(optimizer = tf.keras.optimizers.Adam(0.0001),\n","                loss = get_weighted_loss(class_weights),\n","                metrics = ['accuracy',tf.keras.metrics.AUC()])\n","\treturn model\n","input_shapes=[np.array(train[i].tolist()).shape[1:] for i in models]\n","tuned_models = load_all_models(models,input_shapes)\n","# define ensemble model\n","stacked_model = define_stacked_model(tuned_models)"],"metadata":{"id":"tcbEDnoje3e9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train = [np.array(train[i].tolist())for i in models]\n","y_train = np.array(train['label'].tolist())\n","\n","X_val = [np.array(valid[i].tolist()) for i in models]\n","y_val = np.array(valid['label'].tolist())\n","\n","y_train = y_train.astype(np.float32)\n","y_val = y_val.astype(np.float32)\n","\n","stacked_model.fit(X_train, y_train, batch_size = 64, epochs = 50, validation_data = [X_val, y_val],\n","                    callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=20)], shuffle=True)\n","stacked_model.save(\"stacked_model3.h5\")\n"],"metadata":{"id":"CzzmoyHxfV4L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"2oh39r9DJeCx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## test model"],"metadata":{"id":"3txxImoQJe-n"}},{"cell_type":"code","source":["test_features['bert_features'] = list(np.array(test_features['bert_features'].to_list())[:,0,:].reshape(-1,768,1))\n"],"metadata":{"id":"f9jL2PZvJiUK"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9GVgj5pmHV1L"},"outputs":[],"source":["X_test = [np.array(test_features[i].tolist()) for i in models]\n","pred = stacked_model.predict(X_test)\n"]},{"cell_type":"code","source":["pred[:30]"],"metadata":{"id":"G13KRiSuqFkl"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vbVQO7TDHcc5"},"outputs":[],"source":["y_pred = [[1 if i>0.5 else 0 for i in n] for n in pred]\n","y_pred[:30]"]},{"cell_type":"code","source":["data_test"],"metadata":{"id":"FiKTQOaWPhsV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.array(y_pred)[:,1]"],"metadata":{"id":"9DGR1FMZP4RD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for index,i in enumerate(labels):\n","  data_test[i] = list(np.array(y_pred)[:,index])\n","data_test.to_csv('claim_final_test4.csv',index=False)\n","pd.read_csv('claim_final_test4.csv')"],"metadata":{"id":"Iyjv7o5vPlhD"},"execution_count":null,"outputs":[]}]}